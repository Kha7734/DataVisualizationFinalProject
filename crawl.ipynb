{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Code For Scraping The Data at Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'html': '<option value=\"CO\" selected>CO</option><option value=\"NO2\" selected>NO2</option><option value=\"O3\" selected>O3</option><option value=\"PM-10\" selected>PM-10</option><option value=\"PM-2-5\" selected>PM-2-5</option><option value=\"SO2\" selected>SO2</option>', 'success': True}\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://enviinfo.cem.gov.vn\"\n",
    "endpoint = \"/eip/default/call/json/get_indicators_have_data\"\n",
    "\n",
    "url = base_url + endpoint\n",
    "\n",
    "payload = {\n",
    "    'station_id': 28602553176253587650986727137, \n",
    "    'from_public': 1,\n",
    "    'station_type': 4  \n",
    "}\n",
    "\n",
    "response = requests.post(url, data=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(\"Lỗi:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API For Getting Data At Stations In 1900 Days\n",
    "\n",
    "- AQI API: `/eip/default/call/json/get_aqi_data%3Fdate%3D1900%26aqi_type%3D0` for getting data in 1900 days\n",
    "- Indicators API: `/eip/default/call/json/get_indicators_have_data` for getting the name of datatables columns\n",
    "- Station Details API: `/eip/default/call/json/get_stations_by_province` for getting the `StationID`, `lat`, `lon`, and its `address`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Configuration for the API request\n",
    "api_base_url = \"https://enviinfo.cem.gov.vn\"\n",
    "stations_endpoint = \"/eip/default/call/json/get_stations_by_province\"\n",
    "complete_stations_url = f\"{api_base_url}{stations_endpoint}\"\n",
    "\n",
    "# Detailed request payload for fetching station data\n",
    "stations_payload = {\n",
    "    'sEcho': 1,  # Request identifier for DataTables\n",
    "    'iColumns': 3,  # Number of columns in the DataTable\n",
    "    'sColumns': ',,',  # Column identifiers\n",
    "    'iDisplayStart': 0,  # Start point in the data set (for pagination)\n",
    "    'iDisplayLength': 77,  # Number of records to fetch\n",
    "    'mDataProp_0': 0,  # Data property for the first column\n",
    "    'sSearch_0': '',  # Search term for the first column\n",
    "    'bRegex_0': False,  # Use regex for the first column search term?\n",
    "    'bSearchable_0': True,  # Can the first column be searched?\n",
    "    'mDataProp_1': 1,  # Data property for the second column\n",
    "    'station_type': 4  # Filter for station type\n",
    "}\n",
    "\n",
    "# Perform the POST request to fetch station data by province\n",
    "station_response = requests.post(complete_stations_url, data=stations_payload)\n",
    "\n",
    "# Handling the response\n",
    "if station_response.status_code == 200:\n",
    "    # Saving the response data to a JSON file\n",
    "    with open('province-stations/province_station_data.json', 'w') as file:\n",
    "        json.dump(station_response.json(), file)\n",
    "    print(\"Station data saved successfully.\")\n",
    "else:\n",
    "    print(f\"Error fetching station data: {station_response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your base URL and specific endpoints\n",
    "base_url = \"https://enviinfo.cem.gov.vn\"\n",
    "endpoint_aqi_data = \"/eip/default/call/json/get_aqi_data%3Fdate%3D1900%26aqi_type%3D0\"\n",
    "endpoint_indicators_data = \"/eip/default/call/json/get_indicators_have_data\"\n",
    "\n",
    "# Construct complete URLs for the requests\n",
    "url_aqi_data = f\"{base_url}{endpoint_aqi_data}\"\n",
    "url_indicators_data = f\"{base_url}{endpoint_indicators_data}\"\n",
    "\n",
    "# Load province data to get province IDs\n",
    "with open(\"province-stations/province_station_data.json\", \"r\") as file:\n",
    "    province_data = json.load(file)\n",
    "\n",
    "    province_ids = [BeautifulSoup(province[1], \"html.parser\").find(\"a\")[\"data-id\"] for province in province_data['aaData']]\n",
    "    province_latitudes = [BeautifulSoup(province[1], \"html.parser\").find(\"a\")[\"data-lat\"] for province in province_data['aaData']]\n",
    "    province_longitudes = [BeautifulSoup(province[1], \"html.parser\").find(\"a\")[\"data-lon\"] for province in province_data['aaData']]\n",
    "\n",
    "# Initialize payloads for the requests\n",
    "payload_aqi = {\n",
    "    'sEcho': 1,\n",
    "    'iColumns': 9,\n",
    "    'sColumns': ',,,,,,,,',\n",
    "    'iDisplayStart': 0,\n",
    "    'iDisplayLength': 1900,\n",
    "    'mDataProp_0': 0,\n",
    "    'sSearch_0': '',\n",
    "    'bRegex_0': False,\n",
    "    'bSearchable_0': True,\n",
    "}\n",
    "\n",
    "payload_indicators = {\n",
    "    'station_id': 0,  \n",
    "    'from_public': 1,\n",
    "    'station_type': 4,\n",
    "}\n",
    "\n",
    "# Process each province\n",
    "for province_id in province_ids[70:77]:\n",
    "    # Update payloads with the current province ID\n",
    "    payload_aqi[\"station_id\"] = province_id\n",
    "    payload_indicators[\"station_id\"] = province_id\n",
    "\n",
    "    # Fetch available indicators for the current province\n",
    "    response_indicators = requests.post(url_indicators_data, data=payload_indicators)\n",
    "    if response_indicators.status_code == 200:\n",
    "        indicator_html = response_indicators.json().get('html', '')\n",
    "        soup = BeautifulSoup(indicator_html, 'html.parser')\n",
    "        selected_indicators = [option.text for option in soup.find_all('option', selected=True)]\n",
    "        \n",
    "        # Fetch AQI data for the current province\n",
    "        response_aqi = requests.post(url_aqi_data, data=payload_aqi)\n",
    "        if response_aqi.status_code == 200:\n",
    "            data = response_aqi.json()\n",
    "            # Assign the new column names including the indicators\n",
    "            data['aoColumns'] = ['ID', 'Date', 'AQI'] + selected_indicators\n",
    "\n",
    "            # Insert 2 columns to store the (province_lattitude, province_longitude)\n",
    "            data['aoColumns'].insert(1, 'Province Latitude')\n",
    "            data['aoColumns'].insert(2, 'Province Longitude')\n",
    "\n",
    "            # Insert the province latitude and longitude values ignoring the ID and Date columns\n",
    "            data['aaData'] = [[*row[:1], province_latitudes[province_ids.index(province_id)], province_longitudes[province_ids.index(province_id)], *row[1:]] for row in data['aaData']]\n",
    "\n",
    "            # Save the updated data to a file\n",
    "            with open(f\"data-province/test-{province_id}.json\", \"w\") as file:\n",
    "                json.dump(data, file)\n",
    "            # print(f\"Data for province {province_id} saved successfully.\")\n",
    "        else:\n",
    "            print(f\"Error fetching AQI data for province {province_id}: {response_aqi.status_code}\")\n",
    "    else:\n",
    "        print(f\"Error fetching indicators for province {province_id}: {response_indicators.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"province-stations/province_station_data.json\", \"r\") as f:\n",
    "    province_data = json.load(f)\n",
    "    province_names = []\n",
    "    province_ids = []\n",
    "    for province in province_data['aaData']:\n",
    "        html_snippet = province[1]\n",
    "        soup = BeautifulSoup(html_snippet, \"html.parser\")\n",
    "        province_id = soup.find(\"a\")[\"data-id\"]\n",
    "        province_name = soup.find(\"a\").get_text()\n",
    "\n",
    "        province_name = province_name.replace(\":\", \"\").replace(\"/\", \"-\").replace(\"\\\\\", \"-\")\n",
    "        \n",
    "        # Find the file with their data-id then change to the province name if exists, ifnot, ignore\n",
    "        try:\n",
    "            with open(f\"data-province/test-{province_id}.json\", \"r\") as file:\n",
    "                data = json.load(file)\n",
    "                with open(f\"data-provinces-name/{province_name}.json\", \"w\") as new_file:\n",
    "                    json.dump(data, new_file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File with data-id {province_id} not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data for province {province_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for HCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCM_station_data = {\n",
    "  '1' : {\n",
    "    'Address': 'DHQG, Linh Trung, Thủ Đức',\n",
    "    'Province': 'TP HCM',\n",
    "    'Province Latitude': '10.86994333',\n",
    "    'Province Longitude': '106.7960143'\n",
    "  },\n",
    "  '2' : {\n",
    "    'Address': '20, Nguyễn Trọng Trí, An Lạc, Bình Tân',\n",
    "    'Province': 'TP HCM',\n",
    "    'Province Latitude': '10.74097081',\n",
    "    'Province Longitude': '106.6204143'\n",
    "  },\n",
    "  '3' : {\n",
    "    'Address': 'KCN Tân Bình, Tây Thạnh, Tân Phú',\n",
    "    'Province': 'TP HCM',\n",
    "    'Province Latitude': '10.81621227',\t\n",
    "    'Province Longitude': '106.6204143'\n",
    "  },\n",
    "  '4' : {\n",
    "    'Address': '49, Thanh Đa, P.27, Bình Thạnh',\n",
    "    'Province': 'TP HCM',\n",
    "    'Province Latitude': '10.81584553',\t\n",
    "    'Province Longitude': '106.7174282'\n",
    "  },\n",
    "  '5' : {\n",
    "    'Address': '268, Nguyễn Đình Chiểu, P.6, Q.3',\n",
    "    'Province': 'TP HCM',\n",
    "    'Province Latitude': '10.77636612',\t\t\n",
    "    'Province Longitude': '106.6878094'\n",
    "  },\n",
    "  '6' : {\n",
    "    'Address': 'MM18, Trường Sơn, P.14, Q.10',\n",
    "    'Province': 'TP HCM',\n",
    "    'Province Latitude': '10.78047163',\t\n",
    "    'Province Longitude': '106.6594579'\n",
    "  }\n",
    "}\n",
    "\n",
    "# Read the file temp_data/data/aqi_hcm_2021_2022.csv\n",
    "df_hcm = pd.read_csv(\"temp_data/data/aqi_hcm_2021_2022.csv\")\n",
    "# Convert the Station_No base on the HCM_station_data\n",
    "# Add the Province, Province Latitude, Province Longitude, Address columns\n",
    "df_hcm[\"Station_No\"] = df_hcm[\"Station_No\"].astype(str)\n",
    "df_hcm[\"Province\"] = df_hcm[\"Station_No\"].map(lambda x: HCM_station_data[x][\"Province\"])\n",
    "df_hcm[\"Province Latitude\"] = df_hcm[\"Station_No\"].map(lambda x: HCM_station_data[x][\"Province Latitude\"])\n",
    "df_hcm[\"Province Longitude\"] = df_hcm[\"Station_No\"].map(lambda x: HCM_station_data[x][\"Province Longitude\"])\n",
    "df_hcm[\"Address\"] = df_hcm[\"Station_No\"].map(lambda x: HCM_station_data[x][\"Address\"])\n",
    "\n",
    "# Rename date to Date\n",
    "df_hcm.rename(columns={\"date\": \"Date\"}, inplace=True)\n",
    "\n",
    "# Re-order the columns\n",
    "df_hcm = df_hcm[[\"Station_No\", \"Province\", \"Address\", \"Province Latitude\", \"Province Longitude\", \"Date\", \"TSP\" ,\"PM2.5\", \"O3\", \"CO\", \"NO2\", \"SO2\",\"Temperature\", \"Humidity\"]]\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "# df_hcm.to_csv(\"aqi_hcm_2021_2022_updated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station_No                0\n",
      "Province                  0\n",
      "Address                   0\n",
      "Province Latitude         0\n",
      "Province Longitude        0\n",
      "Date                      0\n",
      "TSP                      60\n",
      "PM2.5                     0\n",
      "O3                    10610\n",
      "CO                     9065\n",
      "NO2                    5666\n",
      "SO2                   11006\n",
      "Temperature            4437\n",
      "Humidity               4432\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print missing values of df_hcm\n",
    "print(df_hcm.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing value with the mean of the column of each station No\n",
    "for station_no in df_hcm[\"Station_No\"].unique():\n",
    "    for column in df_hcm.columns[6:]:\n",
    "        df_hcm.loc[df_hcm[\"Station_No\"] == station_no, column] = df_hcm.loc[df_hcm[\"Station_No\"] == station_no, column].fillna(df_hcm.loc[df_hcm[\"Station_No\"] == station_no, column].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station_No            0\n",
      "Province              0\n",
      "Address               0\n",
      "Province Latitude     0\n",
      "Province Longitude    0\n",
      "Date                  0\n",
      "TSP                   0\n",
      "PM2.5                 0\n",
      "O3                    0\n",
      "CO                    0\n",
      "NO2                   0\n",
      "SO2                   0\n",
      "Temperature           0\n",
      "Humidity              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print missing values of df_hcm after filling missing values\n",
    "print(df_hcm.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define breakpoints and AQI levels\n",
    "breakpoints = {\n",
    "    'O3': [(0, 54), (55, 70), (71, 85), (86, 105), (106, 200)],\n",
    "    'PM2.5': [(0.0, 12.0), (12.1, 35.4), (35.5, 55.4), (55.5, 150.4), (150.5, 250.4), (250.5, 350.4), (350.5, 500.4)],\n",
    "    'CO': [(0.0, 4.4), (4.5, 9.4), (9.5, 12.4), (12.5, 15.4), (15.5, 30.4), (30.5, 40.4), (40.5, 50.4)],\n",
    "    'SO2': [(0, 35), (36, 75), (76, 185), (186, 304)],\n",
    "    'NO2': [(0, 53), (54, 100), (101, 360), (361, 649), (650, 1249), (1250, 1649), (1650, 2049)]\n",
    "}\n",
    "aqi_levels = [(0, 50), (51, 100), (101, 150), (151, 200),\n",
    "              (201, 300), (301, 400), (401, 500)]\n",
    "\n",
    "def calculate_aqi(parameter, value, breakpoints):\n",
    "    for index, (low, high) in enumerate(breakpoints):\n",
    "        if low <= value <= high:\n",
    "            aqi = round(((aqi_levels[index][1] - aqi_levels[index][0]) / (high - low)) * (value - low) + aqi_levels[index][0])\n",
    "            return aqi\n",
    "    return 0  # Return a default value if outside any breakpoint range\n",
    "\n",
    "# Calculate total AQI for each row\n",
    "df_hcm[\"AQI\"] = df_hcm.apply(lambda x: max([calculate_aqi(param, x[param], breakpoints[param]) for param in [\"O3\", \"PM2.5\", \"CO\", \"SO2\", \"NO2\"] if x[param] is not None]), axis=1)\n",
    "\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_hcm.to_csv(\"aqi_hcm_2021_2022_updated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcm.to_csv(\"final-data/aqi_hcm_2021_2022_updated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing for All Provinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame with predefined columns\n",
    "df_columns = [\"Province\", \"Province Latitude\", \"Province Longitude\", \"Date\", \"AQI\", \"CO\", \"NO2\", \"O3\", \"PM-10\", \"PM-2-5\", \"SO2\"]\n",
    "df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "# Path to the folder containing the data files\n",
    "folder_path = \"data-provinces-name\"\n",
    "\n",
    "# Define a function to handle missing or \"-\" values\n",
    "def replace_missing(value):\n",
    "    if value == \"-\":\n",
    "        return np.nan\n",
    "    return value\n",
    "\n",
    "# Process each JSON file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "            # Extract province name from the filename\n",
    "            province_name = filename[:-5]  # Assuming '.json' is the extension\n",
    "\n",
    "            # Assuming 'aoColumns' contains column names directly\n",
    "            # 'aaData' is the list of data rows\n",
    "            file_columns = data.get(\"aoColumns\", [])\n",
    "            aqi_data = data.get(\"aaData\", [])\n",
    "\n",
    "            # Prepare data for DataFrame creation\n",
    "            temp_data = []\n",
    "            for row in aqi_data:\n",
    "                # Replace \"-\" with np.nan and ensure the length matches file_columns\n",
    "                row_data = [replace_missing(item) for item in row] + [np.nan] * (len(file_columns) - len(row))\n",
    "                temp_data.append([province_name] + row_data[:len(file_columns)])\n",
    "\n",
    "            # Create DataFrame for the current file\n",
    "            temp_df = pd.DataFrame(temp_data, columns=[\"Province\"] + file_columns)\n",
    "\n",
    "            # Ensure all predefined columns are present, adding missing ones with np.nan\n",
    "            for column in df_columns:\n",
    "                if column not in temp_df.columns:\n",
    "                    temp_df[column] = np.nan\n",
    "\n",
    "            # Concatenate with the main DataFrame\n",
    "            df = pd.concat([df, temp_df[df_columns]], ignore_index=True)\n",
    "\n",
    "# Convert numeric columns to float, handling errors (like conversion failures) by ignoring them\n",
    "numeric_columns = [\"AQI\", \"CO\", \"NO2\", \"O3\", \"PM-10\", \"PM-2-5\", \"SO2\"]\n",
    "df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Convert '%d/%m/%Y' date format to 'yyyy-mm-dd'\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%d/%m/%Y\").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Save the combined DataFrame to CSV\n",
    "df.to_csv(\"combined_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the combined data from the CSV file\n",
    "df = pd.read_csv(\"combined_data.csv\")\n",
    "\n",
    "# Convert the name of 'Province' column to 'Address'\n",
    "df.rename(columns={\"Province\": \"Address\"}, inplace=True)\n",
    "\n",
    "df[\"Address\"] = df[\"Address\"].str.replace(\"Thừa Thiên Huế\", \"TT Huế\")\n",
    "df[\"Address\"] = df[\"Address\"].str.replace(\"HCM\", \"TP HCM\")\n",
    "\n",
    "# Separate the 'Address' column into 'Province' and 'Address' columns\n",
    "# The Province will get 2 first words and the Address will get the rest\n",
    "df[\"Province\"] = df[\"Address\"].str.split(\" \").str[:2].str.join(\" \")\n",
    "df[\"Address\"] = df[\"Address\"].str.split(\" \").str[2:].str.join(\" \")\n",
    "\n",
    "# Re-order the columns\n",
    "df = df[[\"Province\", \"Address\", \"Province Latitude\", \"Province Longitude\", \"Date\", \"AQI\", \"CO\", \"NO2\", \"O3\", \"PM-10\", \"PM-2-5\", \"SO2\"]]\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(\"combined_data_updated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thanh Hoá' 'Hưng Yên' 'Hà Nam' 'Bắc Ninh' 'Thái Bình' 'Vũng Tàu'\n",
      " 'Trà Vinh' 'Quảng Ngãi' 'Hà Nội' 'Quảng Ninh' 'TT Huế' 'Gia Lai'\n",
      " 'Hậu Giang' 'Long An' 'Bắc Giang' 'Đà Nẵng' 'Bình Phước' 'Nghệ An'\n",
      " 'TP HCM' 'Cao Bằng' 'Vĩnh Long' 'Hải Dương' 'Bình Dương' 'Ninh Thuận'\n",
      " 'Lâm Đồng' 'Phú Thọ' 'Quảng Bình' 'Gia lai' 'Bình Định' 'Hà Tĩnh'\n",
      " 'Khánh Hòa' 'Quảng Nam' 'Lào Cai' 'Lạng Sơn']\n",
      "Provinces count:  34\n"
     ]
    }
   ],
   "source": [
    "# Show the distinct value of Province column\n",
    "print(df[\"Province\"].unique())\n",
    "print(\"Provinces count: \", df[\"Province\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55774 entries, 0 to 55773\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Province            55774 non-null  object \n",
      " 1   Address             55774 non-null  object \n",
      " 2   Province Latitude   55774 non-null  float64\n",
      " 3   Province Longitude  55774 non-null  float64\n",
      " 4   Date                55774 non-null  object \n",
      " 5   AQI                 55774 non-null  int64  \n",
      " 6   CO                  44432 non-null  float64\n",
      " 7   NO2                 44570 non-null  float64\n",
      " 8   O3                  44187 non-null  float64\n",
      " 9   PM-10               52606 non-null  float64\n",
      " 10  PM-2-5              52075 non-null  float64\n",
      " 11  SO2                 46264 non-null  float64\n",
      "dtypes: float64(8), int64(1), object(3)\n",
      "memory usage: 5.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print the properties of the DataFrame\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Province                  0\n",
      "Address                   0\n",
      "Province Latitude         0\n",
      "Province Longitude        0\n",
      "Date                      0\n",
      "AQI                       0\n",
      "CO                    11342\n",
      "NO2                   11204\n",
      "O3                    11587\n",
      "PM-10                  3168\n",
      "PM-2-5                 3699\n",
      "SO2                    9510\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of missing values in each column\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df[\"Month\"] = df[\"Date\"].dt.month\n",
    "df[\"Year\"] = df[\"Date\"].dt.year\n",
    "\n",
    "def fill_by_same_month_year(group):\n",
    "    \"\"\"Fill missing values with the mean of the same month in the same year of the province.\"\"\"\n",
    "    numeric_cols = group.select_dtypes(include=[np.number]).columns.difference(['Year', 'Month'])\n",
    "    for col in numeric_cols:\n",
    "        if col not in ['Province', 'Date', 'Year']:\n",
    "            group[col] = group[col].fillna(group[col].mean())\n",
    "    return group\n",
    "\n",
    "df = fill_by_same_month_year(df)\n",
    "\n",
    "df.drop(columns=[\"Year\", \"Month\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9274\n"
     ]
    }
   ],
   "source": [
    "# Count lines has year before 2020 and after 2023\n",
    "print(df[(df[\"Date\"] < \"2020-01-01\") | (df[\"Date\"] > \"2023-12-31\")].shape[0])\n",
    "\n",
    "# Drop\n",
    "df = df[(df[\"Date\"] >= \"2020-01-01\") & (df[\"Date\"] <= \"2023-12-31\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Province              0\n",
      "Address               0\n",
      "Province Latitude     0\n",
      "Province Longitude    0\n",
      "Date                  0\n",
      "AQI                   0\n",
      "CO                    0\n",
      "NO2                   0\n",
      "O3                    0\n",
      "PM-10                 0\n",
      "PM-2-5                0\n",
      "SO2                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this updated DataFrame to a new CSV file\n",
    "df.to_csv(\"final-data/combined_data_updated_filled.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
